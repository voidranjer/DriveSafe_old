{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19589082-f9dc-44e8-aec7-6951141d7090",
   "metadata": {},
   "source": [
    "# Train the DriveSafe model with RNN (LSTM)\n",
    "\n",
    "Takes fixed-length video snippets as inputs and produces a label output\n",
    "\n",
    "- Driving simulator [here](https://www.crazygames.com/game/city-car-driving-simulator)\n",
    "- Built based on this [towardsdatascience article](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)\n",
    "- [pyimagesearch](https://pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/) article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5a3415-529f-418d-ba57-841056b38a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1269a6cd-0b86-4642-a8a4-700322812a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cd9652-e99e-4b6f-b526-0e269ee832b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "from utils.data import get_sequence_data\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Flatten, Input, AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe429b6-7b8d-4794-bb79-db92d4291d42",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb73130f-9a83-4a23-96bd-cce7b25c938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config vars\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "OUTPUT_PATH = \"model\"\n",
    "FRAMES_PER_SEQ = 30\n",
    "LABELS = set([\"collision\", \"safe\", \"tailgating\", \"weaving\"])\n",
    "\n",
    "NUM_EPOCHS = 25 #150\n",
    "BATCH_SIZE = 32 #2048\n",
    "PATIENCE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c43aff-b12d-41b2-b1e5-6685d590b44a",
   "metadata": {},
   "source": [
    "# Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efa0596-b1a9-48f4-acde-407fdec772e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading sequence data...\n",
      "[INFO] number of sequences in training_data: 246\n",
      "[INFO] number of sequences in validation_data: 24\n"
     ]
    }
   ],
   "source": [
    "# load sequence data\n",
    "print(\"[INFO] loading sequence data...\")\n",
    "data, labels = get_sequence_data(DATASET_PATH, LABELS, FRAMES_PER_SEQ)\n",
    "\n",
    "# convert the data and labels to numpy arrays\n",
    "training_data = np.array(data[\"training\"])\n",
    "training_labels = np.array(labels[\"training\"])\n",
    "validation_data = np.array(data[\"validation\"])\n",
    "validation_labels = np.array(labels[\"validation\"])\n",
    "\n",
    "# count number of sequences\n",
    "print(f\"[INFO] number of sequences in training_data: {len(training_data)}\")\n",
    "print(f\"[INFO] number of sequences in validation_data: {len(validation_data)}\")\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "training_labels = lb.fit_transform(training_labels)\n",
    "validation_labels = lb.transform(validation_labels) # use transform instead of fit_transform because we want to use the same encoder as the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ce070-e72d-4159-a215-5c48c93f2913",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "We want to reshape trainX like this: (num_sequences, FRAMES_PER_SEQ, 224, 224, 3) --> (num_sequences, FRAMES_PER_SEQ, IMAGE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5539fcd-0e5e-47c1-a2f7-21cad6b99965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits\n",
    "# (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)\n",
    "trainX = training_data\n",
    "testX = validation_data\n",
    "trainY = training_labels\n",
    "testY = validation_labels\n",
    "\n",
    "# Load ResNet50 model without the top layer\n",
    "# resnet50_model = ResNet50(weights=\"imagenet\", include_top=False)\n",
    "# def preprocess_sequence(sequence):\n",
    "#     # Preprocess each image in the sequence and extract features using ResNet50\n",
    "#     features_sequence = [resnet50_model.predict(np.expand_dims(img, axis=0)) for img in sequence]\n",
    "#     return features_sequence\n",
    "\n",
    "\n",
    "# trainX has the shape (num_sequences, FRAMES_PER_SEQ, 224, 224, 3)\n",
    "# trainX = trainX.reshape(trainX.shape[0], FRAMES_PER_SEQ, -1)  # Reshaping each sequence of N frames into a single time step\n",
    "# testX = testX.reshape(testX.shape[0], FRAMES_PER_SEQ, -1) # equiv to saying `testX.reshape(NUM_SEQUENCES_IN_TEST_X, FRAMES_PER_SEQ, -1)`\n",
    "# trainX_resnet = [preprocess_sequence(sequence) for sequence in trainX]\n",
    "# testX_resnet = [preprocess_sequence(sequence) for sequence in testX]\n",
    "\n",
    "\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "\trotation_range=30,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "# initialize the validation/testing data augmentation object (which we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the the mean subtraction value for each of the data augmentation objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f5717-42d7-4d4d-8669-b8605541bf27",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071065b-7270-4cfd-b7fa-8ba55303d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:34: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 20s/step - accuracy: 0.2277 - loss: 1.6526 - val_accuracy: 0.3750 - val_loss: 1.4435\n",
      "Epoch 2/25\n",
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m33s\u001b[0m 17s/step - accuracy: 0.2556 - loss: 1.5229"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input,AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# load the ResNet-50 network, ensuring the head FC layer sets are left off\n",
    "baseModel = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will *not* be updated during the training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# construct the head of the model that will be placed on top of the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "# headModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel) # output layer\n",
    "# headModel = Flatten()(headModel) # flatten cnn output to fit LSTM input shape\n",
    "\n",
    "# place the head FC model on top of the base model (this will become the actual model we will train)\n",
    "cnn_model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# Define LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(FRAMES_PER_SEQ, headModel.shape[1])))\n",
    "lstm_model.add(LSTM(128, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Combine the ResNet (cnn_model) and LSTM models\n",
    "combined_model = Sequential()\n",
    "combined_model.add(Input(shape=((None, FRAMES_PER_SEQ, 224, 224, 3))))\n",
    "combined_model.add(TimeDistributed(cnn_model)) # https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "# combined_model.add(TimeDistributed(cnn_model, input_shape=baseModel.input_shape[1:]))\n",
    "combined_model.add(lstm_model)\n",
    "lstm_model.add(Dense(len(lb.classes_), activation=\"softmax\")) # output layer\n",
    "\n",
    "# initialize the model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(lb.classes_), activation=\"softmax\"))\n",
    "\n",
    "# callbacks\n",
    "# patience sets how many epochs to run for before stopping (if no more improvements are made)\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=PATIENCE),\n",
    "#              ModelCheckpoint(os.path.join(OUTPUT_PATH, 'checkpoint.keras'), save_best_only=True, save_weights_only=False)]\n",
    "callbacks = []\n",
    "\n",
    "# compile the model\n",
    "# combined_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=1e-4, momentum=0.9, decay=1e-4 / NUM_EPOCHS)\n",
    "combined_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = combined_model.fit(trainX, trainY, validation_data=(testX, testY), callbacks=callbacks, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
    "# history =  combined_model.fit(\n",
    "# \tx=trainAug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "# \tsteps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "# \tvalidation_data=valAug.flow(testX, testY),\n",
    "# \tvalidation_steps=len(testX) // BATCH_SIZE,\n",
    "# \tepochs=NUM_EPOCHS)\n",
    "NUM_EPOCHS = len(history.history['loss'])\n",
    "\n",
    "# print model summary\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40346d-a6c8-4990-a7e8-66ba08fa7e97",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147e6d5-752b-48ba-a6c0-febea7c81389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating model...\")\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, NUM_EPOCHS), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, \"plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701e3ee-b3f0-4def-a469-a5380789b2fc",
   "metadata": {},
   "source": [
    "# Save Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d827cf-40c5-4a77-af16-f2bb6da61339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing model...\")\n",
    "model.save(os.path.join(OUTPUT_PATH, \"model.keras\"))\n",
    "\n",
    "# serialize the label binarizer to disk\n",
    "with open(os.path.join(OUTPUT_PATH, \"labels.pickle\"), \"wb\") as f:\n",
    "    f.write(pickle.dumps(lb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
