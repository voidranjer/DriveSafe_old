{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19589082-f9dc-44e8-aec7-6951141d7090",
   "metadata": {},
   "source": [
    "# Train the DriveSafe model with RNN (LSTM)\n",
    "\n",
    "- Takes fixed-length video snippets as inputs and produces a label output\n",
    "- Built based on this [towaradsdatascience article](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5a3415-529f-418d-ba57-841056b38a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1269a6cd-0b86-4642-a8a4-700322812a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cd9652-e99e-4b6f-b526-0e269ee832b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "from utils.data import get_sequence_data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Flatten\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe429b6-7b8d-4794-bb79-db92d4291d42",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb73130f-9a83-4a23-96bd-cce7b25c938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config vars\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "OUTPUT_PATH = \"model\"\n",
    "NUM_EPOCHS = 25\n",
    "NUM_FRAMES = 2 # frames per sequence\n",
    "LABELS = set([\"off\", \"collision\", \"safe\", \"tailgating\", \"weaving\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c43aff-b12d-41b2-b1e5-6685d590b44a",
   "metadata": {},
   "source": [
    "# Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8efa0596-b1a9-48f4-acde-407fdec772e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading sequence data...\n",
      "[INFO] number of images in training_data: 151\n",
      "[INFO] number of images in validation_data: 151\n"
     ]
    }
   ],
   "source": [
    "# load sequence data\n",
    "print(\"[INFO] loading sequence data...\")\n",
    "data, labels = get_sequence_data(DATASET_PATH, LABELS, NUM_FRAMES)\n",
    "\n",
    "# convert the data and labels to numpy arrays\n",
    "training_data = np.array(data[\"training\"])\n",
    "training_labels = np.array(labels[\"training\"])\n",
    "validation_data = np.array(data[\"validation\"])\n",
    "validation_labels = np.array(labels[\"validation\"])\n",
    "\n",
    "# count number of sequences\n",
    "print(f\"[INFO] number of images in training_data: {len(training_data)}\")\n",
    "print(f\"[INFO] number of images in validation_data: {len(validation_data)}\")\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "training_labels = lb.fit_transform(training_labels)\n",
    "validation_labels = lb.transform(validation_labels) # use transform instead of fit_transform because we want to use the same encoder as the training data\n",
    "\n",
    "# partition the data into training and testing splits\n",
    "# (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)\n",
    "trainX = training_data\n",
    "testX = validation_data\n",
    "trainY = training_labels\n",
    "testY = validation_labels\n",
    "\n",
    "# trainX has the shape (NUM_FRAMES, 224, 224, 3)\n",
    "# Reshape the input data to fit LSTM's input requirements\n",
    "num_sequence\n",
    "trainX = trainX.reshape(], NUM_FRAMES, -1)  # Reshaping each sequence of N frames into a single time step\n",
    "testX = testX.reshape(testX.shape[0], NUM_FRAMES, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f5717-42d7-4d4d-8669-b8605541bf27",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8071065b-7270-4cfd-b7fa-8ba55303d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_2_1/Cast:0\", shape=(None, 2, 224, 224, 3), dtype=float32). Expected shape (None, 2, 224), but input has incompatible shape (None, 2, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 2, 224, 224, 3), dtype=uint8)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/keras/src/models/functional.py:285\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_2_1/Cast:0\", shape=(None, 2, 224, 224, 3), dtype=float32). Expected shape (None, 2, 224), but input has incompatible shape (None, 2, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 2, 224, 224, 3), dtype=uint8)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(lb.classes_), activation=\"softmax\"))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=NUM_EPOCHS, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40346d-a6c8-4990-a7e8-66ba08fa7e97",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147e6d5-752b-48ba-a6c0-febea7c81389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating model...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, args[\"epochs\"]), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, args[\"epochs\"]), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, args[\"epochs\"]), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, args[\"epochs\"]), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701e3ee-b3f0-4def-a469-a5380789b2fc",
   "metadata": {},
   "source": [
    "# Save Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d827cf-40c5-4a77-af16-f2bb6da61339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing model...\")\n",
    "model.save(args[\"model\"])\n",
    "\n",
    "# serialize the label binarizer to disk\n",
    "with open(args[\"label_bin\"], \"wb\") as f:\n",
    "    f.write(pickle.dumps(lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e3b2a-bb89-4ede-b370-cba9f1952c41",
   "metadata": {},
   "source": [
    "# Early-stopping and Checkpoints\n",
    "Patience sets how many epochs to run for before stopping (if no more improvements are made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28dee34f-33f9-48b8-b8da-84c6693e9ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('./model/checkpoint.keras', save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6aea3e-fe60-41be-9484-db67c71acc54",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955eaaf2-b9d7-4cc7-817e-3c32ed2c2729",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train,  y_train, \n\u001b[1;32m      2\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m      4\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,  y_train, \n",
    "                    batch_size=2048, epochs=150,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
