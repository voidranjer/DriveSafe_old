{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547c868a-2baa-49ef-97fa-678c09168300",
   "metadata": {},
   "source": [
    "# Predict from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cae120a-fc4f-4e49-a74e-784a02551bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from utils.frames import count_frames\n",
    "from utils.plot import create_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd94ea-049a-4ace-a52d-6a96c06a2727",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385b868d-5cc4-4d27-8cd7-14e9220fdc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"safe\", \"collision\", \"tailgating\", \"weaving\"]\n",
    "FRAMES_PER_SEQ = 30\n",
    "FRAME_INTERVAL = 1 # todo: implement intervals here\n",
    "AVG_SIZE = 1\n",
    "\n",
    "VIDEO_PATH = \"input/3.mp4\"\n",
    "MODEL_PATH = \"model\"\n",
    "OUTPUT_PATH = \"output/v3.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300eb236-123c-4c74-a759-845a47369fcb",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc20f67-265f-402b-bc17-4479ef4f20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model and label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# load the trained model and label binarizer from disk\n",
    "print(\"[INFO] loading model and label binarizer...\")\n",
    "model = load_model(os.path.join(MODEL_PATH, \"model.keras\"))\n",
    "lb = pickle.loads(open(os.path.join(MODEL_PATH, \"labels.pickle\"), \"rb\").read())\n",
    "\n",
    "# predictions queue\n",
    "Q = deque(maxlen=AVG_SIZE)\n",
    "\n",
    "# initialize the video stream\n",
    "vs = cv2.VideoCapture(VIDEO_PATH)\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "\n",
    "# initialize stats variables\n",
    "num_frames = count_frames(VIDEO_PATH)\n",
    "prediction_counts = { label: [0] for label in LABELS }\n",
    "current_frame = 0\n",
    "max_y = 0\n",
    "# shape = (FRAMES_PER_SEQ, 224, 224, 3)\n",
    "sequence = []\n",
    "label = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17625369-cc0c-49e6-b09b-24927b16f4dc",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa62389-70f3-4bd9-b2ab-0ddf12ceb5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "PREDICTION:  collision\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "PREDICTION:  collision\n"
     ]
    }
   ],
   "source": [
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    "    \n",
    "    # if the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "    \n",
    "    # if the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "    \n",
    "    # increment the current frame\n",
    "    current_frame += 1\n",
    "        \n",
    "    # process and add the frame to the sequence buffer\n",
    "    output = frame.copy() # secure a copy before modifying the frame to prepare it for prediction\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Is this really necessary??\n",
    "    frame = cv2.resize(frame, (224, 224))  # Resize frame if necessary\n",
    "    frame = np.array(frame)\n",
    "    sequence.append(frame)\n",
    "    \n",
    "    # If we have enough frames, append the sequence to the data and labels\n",
    "    if len(sequence) == FRAMES_PER_SEQ:  # Assuming N frames per sequence\n",
    "        \n",
    "        # make predictions on the frame and then update the predictions queue\n",
    "        to_predict = np.array(sequence).reshape(FRAMES_PER_SEQ, -1)\n",
    "        preds = model.predict(np.expand_dims(to_predict, axis=0))[0]\n",
    "        Q.append(preds)\n",
    "\n",
    "        # reset sequence buffer for next frame\n",
    "        sequence = []\n",
    "\n",
    "        # perform prediction averaging over the current history of previous predictions\n",
    "        results = np.array(Q).mean(axis=0)\n",
    "        i = np.argmax(results)\n",
    "        label = lb.classes_[i]\n",
    "\n",
    "        print(\"PREDICTION: \", label)\n",
    "\n",
    "   # draw the prediction on the output frame\n",
    "    default_color = (100, 100, 100)\n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    text_x = W - 200\n",
    "    text_y = H // 2 - 80\n",
    "    cv2.putText(output, \"SAFE\", (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "             green if label == \"safe\" else default_color, 5)\n",
    "    cv2.putText(output, \"COLLISION\", (text_x, text_y + 50), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "             red if label == \"collision\" else default_color, 5)\n",
    "    cv2.putText(output, \"TAILGATING\", (text_x, text_y + 100), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "             red if label == \"tailgating\" else default_color, 5)\n",
    "    cv2.putText(output, \"WEAVING\", (text_x, text_y + 150), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "             red if label == \"weaving\" else default_color, 5)\n",
    "\n",
    "    # initialize video writer if not already init\n",
    "    if writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, 30,\n",
    "            (W, H), True)\n",
    "    \n",
    "    # write the output frame to disk\n",
    "    writer.write(output)\n",
    "    \t    \n",
    "\n",
    "    # show the output image\n",
    "    cv2.imshow(\"Output\", output)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\t\n",
    "\t# # increment the prediction count\n",
    "\t# for l in lb.classes_:\n",
    "\t# \tnew_count = prediction_counts[l][current_frame - 1] + 1 if label == l else prediction_counts[l][current_frame - 1]\n",
    "\t# \tif (new_count > max_y):\n",
    "\t# \t\tmax_y = new_count\n",
    "\t# \tprediction_counts[l].append(new_count)\n",
    "\n",
    " \n",
    "\t\n",
    "\t# # draw the matplotlib plot on the output frame\n",
    "\t# plot_img = create_plot(current_frame + 1, max_y + 30, [\n",
    "\t# \t{\n",
    "\t# \t\t\"y_vals\": prediction_counts[\"safe\"],\n",
    "\t# \t\t\"label\": \"safe\"\n",
    "\t# \t},\n",
    "\t# \t{\n",
    "\t# \t\t\"y_vals\": prediction_counts[\"collision\"],\n",
    "\t# \t\t\"label\": \"collision\"\n",
    "\t# \t},\n",
    "\t# \t{\n",
    "\t# \t\t\"y_vals\": prediction_counts[\"tailgating\"],\n",
    "\t# \t\t\"label\": \"tailgating\"\n",
    "\t# \t},\n",
    "\t# \t{\n",
    "\t# \t\t\"y_vals\": prediction_counts[\"weaving\"],\n",
    "\t# \t\t\"label\": \"weaving\"\n",
    "\t# \t}\n",
    "\t# ])\n",
    "\n",
    "\t# # resize the plot image to fit the output frame, and convert the color profile\n",
    "\t# plot_img = cv2.resize(plot_img, (W // 3, H // 3))\n",
    "\t# plot_img = cv2.cvtColor(plot_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\t# # add plot to the bottom right of the frame\n",
    "\t# output[H - plot_img.shape[0]:, W - plot_img.shape[1]:W] = plot_img\n",
    "\t\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e974428-4494-4ed7-a3c8-8e8ce366b2e8",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614f5872-2f84-411b-9717-eb48e8461b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cleaning up...\n"
     ]
    }
   ],
   "source": [
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
